---
title: "Detection of adulterated samples of Lycium barbarum polysaccharide with Fourier-transform infrared spectroscopy (FTIR)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results = "hide"}
library(data.table)
library(tidyverse)
library(factoextra)
library(pracma)
library(plotly)
library(stats)
library(caret)

df <- read.csv("wolfberry.csv", row.names=NULL)

df[df$Y == 0, 1] = "LBPs"
df[df$Y == 2, 1] = "Dextran"
df[df$Y == 3, 1] = "Maltodextrin"
df[df$Y == 4, 1] = "Starch"
```


## Step 1: PCA 

There are two general methods to perform PCA in R :

- Spectral decomposition which examines the covariances / correlations between variables
- Singular value decomposition which examines the covariances / correlations between individuals

The function princomp() uses the spectral decomposition approach. The functions prcomp() and PCA()[FactoMineR] use the singular value decomposition (SVD).

```{r}
inclusion1 <- paste0("X", c(1:779))

inclusion2 <- paste0("X", c(1038:1870))

inclusion3 <- paste0("X", c(1142:1870))

inclusion4 <- paste0("X", c(1298:1870))
```

```{r}
df_sub <- df[, c("C", "Y", inclusion3)]

df.active <- df_sub %>% filter(Y != 1)
df.active <- df.active %>% filter(C == 0.0 | C == 0.05) 

df.active_x <- df.active %>% select(-c(Y, C))
dim(df.active_x)
```

### PCA on the gradients of the FTIR

We apply the SVD-based PCA to minimize the individual variance. 


```{r}
value_prime <- pracma::gradient(as.numeric(df.active_x[1, ]), h1 = 1)
plot(value_prime, type = "l", main = "Gradient", ylab = "", xlab = "")
```


```{r}
plot(as.numeric(df.active_x[1, ]), type = "l", main = "Original", ylab = "", xlab = "")
```

```{r}
derivative <- data.frame()

for (i in c(1:nrow(df.active_x))) {
  value_prime <- pracma::gradient(as.numeric(df.active_x[i, ]), h1 = 1)
  value_prime_prime <- pracma::gradient(value_prime, h1 = 1)
  #value_prime <- c(value_prime, value_prime_prime)
  derivative <- rbind(derivative, value_prime)
}
```

```{r}
res.pca <- prcomp(derivative, scale = FALSE)
fviz_eig(res.pca, main="", ggtheme = theme_classic())
```

```{r, echo = FALSE, eval = FALSE}
fviz_pca_ind(res.pca,
             axes=c(1,2),
             label="none",
             habillage=df.active$Y,
             addEllipses=TRUE, 
             ellipse.level=0.95
             )
```

### 3D visualization of the PCA results

```{r}
prin_comp <- res.pca
explained_variance_ratio <- summary(prin_comp)[["importance"]]['Proportion of Variance',]
explained_variance_ratio <- 100 * explained_variance_ratio
components <- prin_comp[["x"]]
components <- data.frame(components)
components <- cbind(components, df.active$Y)
components$PC3 <- -components$PC3
components$PC2 <- -components$PC2

axis = list(showline=FALSE,
            zeroline=FALSE,
            gridcolor='#ffff',
            ticklen=4,
            titlefont=list(size=13))

fig <- components %>%
  plot_ly()  %>%
  add_trace(
    type = 'splom',
    dimensions = list(
      list(label=paste('PC 1 (',toString(round(explained_variance_ratio[1],1)),'%)',sep = ''), values=~PC1),
      list(label=paste('PC 2 (',toString(round(explained_variance_ratio[2],1)),'%)',sep = ''), values=~PC2),
      list(label=paste('PC 3 (',toString(round(explained_variance_ratio[3],1)),'%)',sep = ''), values=~PC3),
      list(label=paste('PC 4 (',toString(round(explained_variance_ratio[4],1)),'%)',sep = ''), values=~PC4)
    ),
    color = ~factor(`df.active$Y`), colors = c('#636EFA','#EF553B','#00CC96', '#F2C43D')
  ) %>%
  style(diagonal = list(visible = FALSE)) %>%
  layout(
    legend=list(title=list(text='color')),
    hovermode='closest',
    dragmode= 'select',
    plot_bgcolor='rgba(240,240,240, 0.95)',
    xaxis=list(domain=NULL, showline=F, zeroline=F, gridcolor='#ffff', ticklen=4),
    yaxis=list(domain=NULL, showline=F, zeroline=F, gridcolor='#ffff', ticklen=4),
    xaxis2=axis,
    xaxis3=axis,
    xaxis4=axis,
    yaxis2=axis,
    yaxis3=axis,
    yaxis4=axis
  )

fig
```

```{r}
prin_comp <- res.pca

components <- prin_comp[["x"]]
components <- data.frame(components)

components$PC3 <- -components$PC3
components$PC2 <- -components$PC2
components <- cbind(components, df.active$Y)

tot_explained_variance_ratio <- summary(prin_comp)[["importance"]]['Proportion of Variance',]
tot_explained_variance_ratio <- 100 * sum(tot_explained_variance_ratio)

#tit = 'Total Explained Variance = 99.48'

fig <- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~factor(`df.active$Y`), colors = c('#636EFA','#EF553B','#00CC96', '#F2C43D')) %>%
  add_markers(size = 30)


fig <- fig %>%
  layout(
    scene = list(bgcolor = "#e5ecf6")
)

fig
```

## Step 2: Classification

```{r}
deriv_df.active <- cbind(derivative, Y = df.active$Y)
```

Looks like the data label is slightly imbalanced: 

```{r}
library(rsample)
library(kableExtra)
RNGkind(sample.kind = "Rounding")
set.seed(417)
df.active_wo_c <- deriv_df.active
splitted <- initial_split(data = df.active_wo_c, prop = 0.8)

table(training(splitted)$Y) %>% kable() %>% kable_styling()
prop.table(table(training(splitted)$Y)) %>% kable() %>% kable_styling()
```

Split into the training and testing samples, training 160 samples, testing 40 samples.

```{r, eval = FALSE}
nrow(training(splitted))
nrow(testing(splitted))
```

### Multi-class logistic, Naive Bayes and SVM. 

```{r, results = 'hide'}
train <- training(splitted)
test <- testing(splitted)

# define models to try
models <- c("multinom", "naive_bayes", "svmLinear", "pls")

# set CV control for knn, k-folds
myfolds <- createMultiFolds(train$Y, k = 5, times = 10)
control <- trainControl("repeatedcv", 
                        index = myfolds, 
                        selectionFunction = "oneSE",
                        preProcOptions = list(pcaComp = 10))

# fit models
set.seed(1)

train_models <- lapply(models, function(model){
    #print(model)
    
    if (model != "pls") {
       train(as.factor(Y) ~ ., 
             method = model, 
             data = train, 
             trControl = control, 
             metric = "Accuracy", 
             preProc = c("pca", "scale")) 
    
      } else {
       
      train(as.factor(Y) ~ ., 
            method = model, 
            data = train, 
            trControl = control, 
            metric = "Accuracy",
            preProc = c("scale"))
    }
})

names(train_models) <- models
```


```{r}
# extract elapsed training times
elapsed <- sapply(train_models, function(object)
    object$times$everything["elapsed"])

# extract accuracy from CM in one step without creating a separate predictions vector
acc = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["overall"]]["Accuracy"])
  }
)

macro_sens = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "Sensitivity"], na.rm = TRUE))
  }
)

sensitivity = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "Sensitivity"])
  }
)

macro_precision = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "Precision"], na.rm = TRUE))
  }
)

precision = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "Precision"])
  }
)

# extract F1 by class
F1 = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "F1"])
  }
)

# extract macro F1
F1_M = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "F1"], na.rm = TRUE))
  }
)
```


```{r}
library(kableExtra)

acc %>% kable() %>% kable_styling()

sensitivity %>% kable() %>% kable_styling()

macro_sens %>% kable() %>% kable_styling()

precision %>% kable() %>% kable_styling()

macro_precision  %>% kable() %>% kable_styling()

F1 %>% kable() %>% kable_styling()

F1_M %>% kable() %>% kable_styling()
```

```{r}
#predict(mod1, newdata = test, type = "prob")
```


(Optional) Visualize the derivative

```{r, eval = FALSE}
plot(as.numeric(train[1, 1:ncol(train)-1]), type = "l", main = train[1, "Y"])

plot(as.numeric(train[24, 1:ncol(train)-1]), type = "l", main = train[24, "Y"])

plot(as.numeric(train[2, 1:ncol(train)-1]), type = "l", main = train[2, "Y"])

plot(as.numeric(train[157, 1:ncol(train)-1]), type = "l", main = train[157, "Y"])
```

### Use the rest of the data as the test set

```{r}
df_sub <- df[, c("C", "Y", inclusion3)]

df.active <- df_sub %>% filter(Y != 1)
df.active <- df.active %>% filter(C != 0.05) 

df.active_x <- df.active %>% select(-c(Y, C))
dim(df.active_x)
deriv_df.active <- cbind(derivative, Y = df.active$Y)

derivative <- data.frame()

for (i in c(1:nrow(df.active_x))) {
  value_prime <- pracma::gradient(as.numeric(df.active_x[i, ]), h1 = 1)
  value_prime_prime <- pracma::gradient(value_prime, h1 = 1)
  #value_prime <- c(value_prime, value_prime_prime)
  derivative <- rbind(derivative, value_prime)
}

test <- cbind(derivative, Y = df.active$Y)
```

```{r}
table(test$Y) %>% kable() %>% kable_styling()
```

```{r}
# extract elapsed training times
elapsed <- sapply(train_models, function(object)
    object$times$everything["elapsed"])

# extract accuracy from CM in one step without creating a separate predictions vector
acc = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["overall"]]["Accuracy"])
  }
)

macro_sens = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "Sensitivity"], na.rm = TRUE))
  }
)

sensitivity = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "Sensitivity"])
  }
)

macro_precision = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "Precision"], na.rm = TRUE))
  }
)

precision = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "Precision"])
  }
)

# extract F1 by class
F1 = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(cm[["byClass"]][ , "F1"])
  }
)

# extract macro F1
F1_M = sapply(train_models, function(x){
    pred = predict(x, newdata = test)
    cm = confusionMatrix(pred, reference = as.factor(test$Y))
    return(mean(cm[["byClass"]][ , "F1"], na.rm = TRUE))
  }
)
```

```{r}
acc %>% kable() %>% kable_styling()

sensitivity %>% kable() %>% kable_styling()

macro_sens %>% kable() %>% kable_styling()

precision %>% kable() %>% kable_styling()

macro_precision  %>% kable() %>% kable_styling()

F1 %>% kable() %>% kable_styling()

F1_M %>% kable() %>% kable_styling()
```

## Regression

### Dextran

```{r}
set.seed(417)
df.active_dex <- df %>% filter(Y == "Dextran") %>% select(-Y)
splitted <- initial_split(data = df.active_dex, prop = 0.8)
```

```{r}
train <- training(splitted)
test <- testing(splitted)
```

```{r}
table(train$C) %>% kable() %>% kable_styling()
```

```{r}
# define models to try
models <- c("lm", "lasso", "ranger", "pls")

# set CV control for knn, k-folds
myfolds <- createMultiFolds(train$C, k = 5, times = 10)
control <- trainControl("repeatedcv", 
                        index = myfolds, 
                        selectionFunction = "oneSE",
                        preProcOptions = list(pcaComp = 10))

# fit models
set.seed(1)

train_models <- lapply(models, function(model){
    
    if (model != "pls") {
       train(C ~ ., 
             method = model, 
             data = train, 
             trControl = control, 
             metric = "RMSE", 
             preProc = c("pca", "scale")) 
    
      } else {
       
      train(C ~ ., 
            method = model, 
            data = train, 
            trControl = control, 
            metric = "RMSE",
            preProc = c("scale"))
    }
})

names(train_models) <- models
```

```{r}
# extract elapsed training times
elapsed <- sapply(train_models, function(object)
    object$times$everything["elapsed"])

# extract accuracy from CM in one step without creating a separate predictions vector
rmse = sapply(train_models, function(x){
    pred = predict(x, newdata = test %>% select(-C))
    cm = postResample(pred = pred, obs = test$C)
  }
)

```

```{r}
rmse
```

```{r}
library(ggplot2)
pred <- predict(train_models, newdata = test)$lm
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)
 
g1 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Dextran - Linear regression")

pred <- predict(train_models, newdata = test)$lasso
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g2 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Dextran - LASSO")

pred <- predict(train_models, newdata = test)$ranger
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g3 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Dextran - Random forest")

pred <- predict(train_models, newdata = test)$pls
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g4 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Dextran - Partial least square")

ggpubr::ggarrange(g1, g2, g3, g4)
```

### Maltodextrin

```{r}
set.seed(417)
df.active_dex <- df %>% filter(Y == "Maltodextrin") %>% select(-Y)
splitted <- initial_split(data = df.active_dex, prop = 0.8)

train <- training(splitted)
test <- testing(splitted)
```

```{r}
# define models to try
models <- c("lm", "lasso", "ranger", "pls")

# set CV control for k-folds
myfolds <- createMultiFolds(train$C, k = 5, times = 10)
control <- trainControl("repeatedcv", 
                        index = myfolds, 
                        selectionFunction = "oneSE",
                        preProcOptions = list(pcaComp = 10))

# fit models
set.seed(1)

train_models <- lapply(models, function(model){
    print(model)
    
    if (model != "pls") {
       train(C ~ ., 
             method = model, 
             data = train, 
             trControl = control, 
             metric = "RMSE", 
             preProc = c("pca", "scale")) 
    
      } else {
       
      train(C ~ ., 
            method = model, 
            data = train, 
            trControl = control, 
            metric = "RMSE",
            preProc = c("scale"))
    }
})

names(train_models) <- models
```

```{r}
# extract elapsed training times
elapsed <- sapply(train_models, function(object)
    object$times$everything["elapsed"])

# extract accuracy from CM in one step without creating a separate predictions vector
rmse = sapply(train_models, function(x){
    pred = predict(x, newdata = test %>% select(-C))
    cm = postResample(pred = pred, obs = test$C)
  }
)

```

```{r}
rmse
```

```{r}
pred <- predict(train_models, newdata = test)$lm
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)
 
g1 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Maltodextrin - Linear regression")

pred <- predict(train_models, newdata = test)$lasso
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g2 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Maltodextrin - LASSO")

pred <- predict(train_models, newdata = test)$ranger
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g3 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Maltodextrin - Random forest")

pred <- predict(train_models, newdata = test)$pls
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g4 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Maltodextrin - Partial least square")

ggpubr::ggarrange(g1, g2, g3, g4)
```

```{r}
set.seed(417)
df.active_dex <- df %>% filter(Y == "Starch") %>% select(-Y)
splitted <- initial_split(data = df.active_dex, prop = 0.8)

train <- training(splitted)
test <- testing(splitted)
```

```{r}
# define models to try
models <- c("lm", "lasso", "ranger",  "pls")

# set CV control for knn, k-folds
myfolds <- createMultiFolds(train$C, k = 5, times = 10)
control <- trainControl("repeatedcv", 
                        index = myfolds, 
                        selectionFunction = "oneSE",
                        preProcOptions = list(pcaComp = 10))

# fit models
set.seed(1)

train_models <- lapply(models, function(model){
    print(model)
    
    if (model != "pls") {
       train(C ~ ., 
             method = model, 
             data = train, 
             trControl = control, 
             metric = "RMSE", 
             preProc = c("pca", "scale")) 
    
      } else {
       
      train(C ~ ., 
            method = model, 
            data = train, 
            trControl = control, 
            metric = "RMSE",
            preProc = c("scale"))
    }
})

names(train_models) <- models
```

```{r}
# extract elapsed training times
elapsed <- sapply(train_models, function(object)
    object$times$everything["elapsed"])

# extract accuracy from CM in one step without creating a separate predictions vector
rmse = sapply(train_models, function(x){
    pred = predict(x, newdata = test %>% select(-C))
    cm = postResample(pred = pred, obs = test$C)
  }
)

```

```{r}
rmse
```

```{r}
pred <- predict(train_models, newdata = test)$lm
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)
 
g1 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Starch - Linear regression")

pred <- predict(train_models, newdata = test)$lasso
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g2 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Starch - LASSO")

pred <- predict(train_models, newdata = test)$ranger
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g3 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Starch - Random forest")

pred <- predict(train_models, newdata = test)$pls
plot_table <- data.frame(pred = pred, test = test$C) 
t.test(plot_table$pred, plot_table$test)

g4 <- plot_table %>% 
  ggplot() +
  geom_line(aes(x = seq(0, 1, length.out=60), y = seq(0, 1, length.out=60))) +
  geom_point(aes(x = pred, y = test), size = 0.5) + theme_bw() + 
  xlab("Predicted concentration") + 
  ylab("True concentration") + 
  ggtitle("Starch - Partial least square")

ggpubr::ggarrange(g1, g2, g3, g4)
```
